{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import json,argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# plot the data\n",
    "import matplotlib.pyplot as plt\n",
    "import natsort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict_everything = {}\n",
    "\n",
    "def get_batch_pin_memory_time_with_ts(log_file: str) -> pd.DataFrame:\n",
    "    with open(log_file) as f:\n",
    "        lines = f.readlines()\n",
    "        batch_pin_memory_times = {}\n",
    "        batch_pin_memory_times_ts = {}\n",
    "        \n",
    "        for line in lines:\n",
    "            if \"SBatchPinMemory\" in line:\n",
    "                parts = line.split(',')\n",
    "                batch_id = int(parts[0].split('_')[1])\n",
    "                pin_memory_time = float(parts[-1]) / (1000 * 1000 * 1000)\n",
    "                batch_pin_memory_times[batch_id] = pin_memory_time\n",
    "                batch_pin_memory_times_ts[batch_id] = float(parts[-2])\n",
    "        \n",
    "        data = {\n",
    "            'batch_id': list(batch_pin_memory_times.keys()),\n",
    "            'pin_memory_time': list(batch_pin_memory_times.values()),\n",
    "            'pin_memory_time_ts': list(batch_pin_memory_times_ts.values())\n",
    "        }\n",
    "        \n",
    "        df = pd.DataFrame(data)\n",
    "        df.set_index('batch_id', inplace=True)\n",
    "        df.sort_index(inplace=True)\n",
    "        \n",
    "        return df\n",
    "\n",
    "def get_batch_idle_times_with_ts(log_file: str) -> pd.DataFrame:\n",
    "    with open(log_file) as f:\n",
    "        lines = f.readlines()\n",
    "        batch_wait_times = {}\n",
    "        batch_wait_times_ts = {}\n",
    "        \n",
    "        for line in lines:\n",
    "            if \"SBatchWait\" in line:\n",
    "                parts = line.split(',')\n",
    "                batch_id = int(parts[0].split('_')[1])\n",
    "                batch_wait_times[batch_id] = float(parts[2]) / (1000 * 1000 * 1000)\n",
    "                batch_wait_times_ts[batch_id] = float(parts[1])\n",
    "        \n",
    "        data = {\n",
    "            'batch_id': list(batch_wait_times.keys()),\n",
    "            'wait_time': list(batch_wait_times.values()),\n",
    "            'wait_time_ts': list(batch_wait_times_ts.values())\n",
    "        }\n",
    "        \n",
    "        df = pd.DataFrame(data)\n",
    "        df.set_index('batch_id', inplace=True)\n",
    "        df.sort_index(inplace=True)\n",
    "        \n",
    "        return df\n",
    "\n",
    "def get_batch_preprocessing_times_with_ts(log_file: str) -> pd.DataFrame:\n",
    "    with open(log_file) as f:\n",
    "        lines = f.readlines()\n",
    "        batch_preprocessing_times = {}\n",
    "        batch_preprocessing_times_ts = {}\n",
    "        \n",
    "        for line in lines:\n",
    "            if \"SBatchPreprocessed\" in line:\n",
    "                parts = line.split(',')\n",
    "                batch_id = int(parts[0].split('_')[1])\n",
    "                preprocessing_time = float(parts[-1]) / (1000 * 1000 * 1000)\n",
    "                batch_preprocessing_times[batch_id] = preprocessing_time\n",
    "                batch_preprocessing_times_ts[batch_id] = float(parts[-2])\n",
    "        \n",
    "        data = {\n",
    "            'batch_id': list(batch_preprocessing_times.keys()),\n",
    "            'preprocessing_time': list(batch_preprocessing_times.values()),\n",
    "            'preprocessing_time_ts': list(batch_preprocessing_times_ts.values())\n",
    "        }\n",
    "        \n",
    "        df = pd.DataFrame(data)\n",
    "        df.set_index('batch_id', inplace=True)\n",
    "        df.sort_index(inplace=True)\n",
    "        \n",
    "        return df\n",
    "\n",
    "def get_batch_consumed_times_with_ts(log_file: str) -> pd.DataFrame:\n",
    "    with open(log_file) as f:\n",
    "        lines = f.readlines()\n",
    "    batch_consumed_times = {}\n",
    "    batch_consumed_times_ts = {}\n",
    "    for line in lines:\n",
    "        if \"SBatchConsumed\" in line:\n",
    "            parts = line.split(',')\n",
    "            batch_id = int(parts[0].split('_')[1])\n",
    "            consumed_time = float(parts[-1]) / (1000 * 1000 * 1000)\n",
    "            batch_consumed_times[batch_id] = consumed_time\n",
    "            batch_consumed_times_ts[batch_id] = int(parts[-2])\n",
    "    \n",
    "    data = {\n",
    "        'batch_id': list(batch_consumed_times.keys()),\n",
    "        'consumed_time': list(batch_consumed_times.values()),\n",
    "        'consumed_time_ts': list(batch_consumed_times_ts.values())\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    df.set_index('batch_id', inplace=True)\n",
    "    df.sort_index(inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_gpu_util_time(gpu_util_file: str) -> pd.DataFrame:\n",
    "    util_times = []\n",
    "    batch_id = 1\n",
    "\n",
    "    with open(gpu_util_file) as f:\n",
    "        for line in f:\n",
    "            if \"ms\" in line:\n",
    "                util_time = float(line.split()[0]) / 1000\n",
    "                util_times.append((batch_id, util_time))\n",
    "                batch_id += 1\n",
    "\n",
    "    df = pd.DataFrame(util_times, columns=['batch_id', 'util_time'])\n",
    "    df.set_index('batch_id', inplace=True)\n",
    "    df.sort_index(inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_gpu_wait_time(gpu_file: str) -> pd.DataFrame:\n",
    "    idle_times = []\n",
    "    batch_id = 1\n",
    "    \n",
    "    with open(gpu_file) as f:\n",
    "        for line in f:\n",
    "            if \"ms\" in line:\n",
    "                idle_time = float(line.split()[0]) / 1000\n",
    "                idle_times.append((batch_id, idle_time))\n",
    "                batch_id += 1\n",
    "    \n",
    "    df = pd.DataFrame(idle_times, columns=['batch_id', 'idle_time'])\n",
    "    df.set_index('batch_id', inplace=True)\n",
    "    df.sort_index(inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_everything(log_files, gpu_file, gpu_util_file):\n",
    "    # get e2e time\n",
    "    df_main = None\n",
    "    for file in log_files:\n",
    "        if \"main\" in file:\n",
    "            # get the batch wait time by ID\n",
    "            df_batch_wait_times = get_batch_idle_times_with_ts(file)\n",
    "            # concat with df_main using batch_id\n",
    "            if df_main is None:\n",
    "                df_main = df_batch_wait_times\n",
    "            else:\n",
    "                df_main = df_main.combine_first(df_batch_wait_times)\n",
    "\n",
    "            df_batch_consumed_times = get_batch_consumed_times_with_ts(file)\n",
    "            # concat with df_main using batch_id\n",
    "            if df_main is None:\n",
    "                df_main = df_batch_consumed_times\n",
    "            else:\n",
    "                df_main = df_main.combine_first(df_batch_consumed_times)\n",
    "            \n",
    "            df_batch_pin_memory_times = get_batch_pin_memory_time_with_ts(file)\n",
    "            # concat with df_main using batch_id\n",
    "            if df_main is None:\n",
    "                df_main = df_batch_pin_memory_times\n",
    "            else:\n",
    "                df_main = df_main.combine_first(df_batch_pin_memory_times)\n",
    "            \n",
    "        if \"worker\" in file:\n",
    "            # get batch preprocessing time by ID\n",
    "            df_batch_preprocessing_times = get_batch_preprocessing_times_with_ts(file)\n",
    "            # concat with df_main using batch_id\n",
    "            if df_main is None:\n",
    "                df_main = df_batch_preprocessing_times\n",
    "            else:\n",
    "                df_main = df_main.combine_first(df_batch_preprocessing_times)\n",
    "    # in df_main, calculate wait_time_ts - (preprocessing_time_ts + preprocessing_time) for each batch and store in a new column\n",
    "    df_main['wait_time_preprocessing_time_ts_diff'] = df_main['wait_time_ts']/(1000 * 1000 * 1000) - (df_main['preprocessing_time_ts']/(1000 * 1000 * 1000) + df_main['preprocessing_time'])\n",
    "    df_main['wait_time_preprocessing_time_ts_diff'] = df_main['wait_time_preprocessing_time_ts_diff']\n",
    "    # in df_main, calculate (consumed_time_ts - (preprocessing_time_ts + preprocessing_time) prefor each batch and store in a new column\n",
    "    df_main['consumed_time_preprocessing_time_ts_diff'] = df_main['consumed_time_ts']/(1000 * 1000 * 1000) - (df_main['preprocessing_time_ts']/(1000 * 1000 * 1000) + df_main['preprocessing_time'])\n",
    "    df_main['consumed_time_preprocessing_time_ts_diff'] = df_main['consumed_time_preprocessing_time_ts_diff']\n",
    "    # get the idle times\n",
    "    df_gpu = get_gpu_wait_time(gpu_file)\n",
    "    df_gpu_util = get_gpu_util_time(gpu_util_file)\n",
    "    # concat with df_main using batch_id\n",
    "    # df_main = pd.concat([df_main,df_gpu],axis=1)\n",
    "    df_main = df_main.combine_first(df_gpu)\n",
    "    df_main = df_main.combine_first(df_gpu_util)\n",
    "    return df_main\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the directory to search for log files\n",
    "dir_ = \"../final_analysis_cloudlab_twenty\"\n",
    "\n",
    "# Function to get GPU and main log files from the specified directory\n",
    "def get_gpu_and_main_log_files(dir_path: str) -> tuple:\n",
    "    import os\n",
    "    gpu_files = []  # List to store GPU idle files\n",
    "    gpu_util_files = []  # List to store GPU utilization files\n",
    "    log_files = {}  # Dictionary to store main and worker log files\n",
    "\n",
    "    # Walk through the directory and its subdirectories\n",
    "    for root, dirs, files in os.walk(dir_path):\n",
    "        log_files[root] = []  # Initialize the list for the current directory\n",
    "        for file in files:\n",
    "            # Check if the file is a GPU utilization file (excluding PNG files)\n",
    "            if \"gpu_util\" in file and 'png' not in file:\n",
    "                gpu_util_files.append(os.path.join(root, file))\n",
    "            # Check if the file is a GPU idle file (excluding PNG files)\n",
    "            if \"gpu_idle\" in file and 'png' not in file:\n",
    "                gpu_files.append(os.path.join(root, file))\n",
    "            # Check if the file is a main log file (excluding PNG files)\n",
    "            if \"main\" in file and 'png' not in file:\n",
    "                log_files[root].append(os.path.join(root, file))\n",
    "            # Check if the file is a worker log file (excluding PNG files)\n",
    "            if \"worker\" in file and 'png' not in file:\n",
    "                log_files[root].append(os.path.join(root, file))\n",
    "\n",
    "    # Return the lists of GPU idle files, main and worker log files, and GPU utilization files\n",
    "    return gpu_files, log_files, gpu_util_files\n",
    "\n",
    "# Get the GPU idle files, main and worker log files, and GPU utilization files from the specified directory\n",
    "gpu_files, log_files, gpu_util_files = get_gpu_and_main_log_files(dir_)\n",
    "\n",
    "# Filter out empty log file entries\n",
    "log_files = {key: val for key, val in log_files.items() if val}\n",
    "\n",
    "# Sort the log file keys naturally\n",
    "log_files_keys = natsort.natsorted(log_files)\n",
    "\n",
    "# Sort the GPU idle files naturally\n",
    "gpu_files = natsort.natsorted(gpu_files)\n",
    "\n",
    "# Sort the GPU utilization files naturally\n",
    "gpu_util_files = natsort.natsorted(gpu_util_files)\n",
    "\n",
    "\n",
    "for key in log_files_keys:\n",
    "    log_files[key] = natsort.natsorted(log_files[key])\n",
    "    print(key,log_files[key])\n",
    "print(gpu_files)\n",
    "print(len(log_files_keys), len(gpu_files), len(gpu_util_files))\n",
    "\n",
    "\n",
    "for key, gpu_file, gpu_util_file in zip(log_files_keys, gpu_files, gpu_util_files):\n",
    "    log_files_list = log_files[key]\n",
    "    # keep only the text after the last '/'\n",
    "    key = key.split('/')[-1]\n",
    "    print(key)\n",
    "    df_dict_everything[key] = get_everything(log_files_list, gpu_file, gpu_util_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict_everything_io_only = {}\n",
    "\n",
    "# Define the directory to search for log files\n",
    "dir_ = \"../final_analysis_cloudlab_io_badalloc_twenty\"\n",
    "\n",
    "# Function to get GPU and main log files from the specified directory\n",
    "def get_gpu_and_main_log_files(dir_path: str) -> tuple:\n",
    "    import os\n",
    "    gpu_files = []  # List to store GPU idle files\n",
    "    gpu_util_files = []  # List to store GPU utilization files\n",
    "    log_files = {}  # Dictionary to store main and worker log files\n",
    "\n",
    "    # Walk through the directory and its subdirectories\n",
    "    for root, dirs, files in os.walk(dir_path):\n",
    "        log_files[root] = []  # Initialize the list for the current directory\n",
    "        for file in files:\n",
    "            # Check if the file is a GPU utilization file (excluding PNG files)\n",
    "            if \"gpu_util\" in file and 'png' not in file:\n",
    "                gpu_util_files.append(os.path.join(root, file))\n",
    "            # Check if the file is a GPU idle file (excluding PNG files)\n",
    "            if \"gpu_idle\" in file and 'png' not in file:\n",
    "                gpu_files.append(os.path.join(root, file))\n",
    "            # Check if the file is a main log file (excluding PNG files)\n",
    "            if \"main\" in file and 'png' not in file:\n",
    "                log_files[root].append(os.path.join(root, file))\n",
    "            # Check if the file is a worker log file (excluding PNG files)\n",
    "            if \"worker\" in file and 'png' not in file:\n",
    "                log_files[root].append(os.path.join(root, file))\n",
    "\n",
    "    # Return the lists of GPU idle files, main and worker log files, and GPU utilization files\n",
    "    return gpu_files, log_files, gpu_util_files\n",
    "\n",
    "# Get the GPU idle files, main and worker log files, and GPU utilization files from the specified directory\n",
    "gpu_files, log_files, gpu_util_files = get_gpu_and_main_log_files(dir_)\n",
    "\n",
    "# Filter out empty log file entries\n",
    "log_files = {key: val for key, val in log_files.items() if val}\n",
    "\n",
    "# Sort the log file keys naturally\n",
    "log_files_keys = natsort.natsorted(log_files)\n",
    "\n",
    "# Sort the GPU idle files naturally\n",
    "gpu_files = natsort.natsorted(gpu_files)\n",
    "\n",
    "# Sort the GPU utilization files naturally\n",
    "gpu_util_files = natsort.natsorted(gpu_util_files)\n",
    "\n",
    "\n",
    "for key in log_files_keys:\n",
    "    log_files[key] = natsort.natsorted(log_files[key])\n",
    "    print(key,log_files[key])\n",
    "print(gpu_files)\n",
    "print(len(log_files_keys), len(gpu_files), len(gpu_util_files))\n",
    "\n",
    "\n",
    "for key, gpu_file, gpu_util_file in zip(log_files_keys, gpu_files, gpu_util_files):\n",
    "    log_files_list = log_files[key]\n",
    "    # keep only the text after the last '/'\n",
    "    key = key.split('/')[-1]\n",
    "    print(key)\n",
    "    df_dict_everything_io_only[key] = get_everything(log_files_list, gpu_file, gpu_util_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict_everything_good_alloc_only = {}\n",
    "\n",
    "# Define the directory to search for log files\n",
    "dir_ = \"../final_analysis_cloudlab_good_alloc_twenty\"\n",
    "\n",
    "# Function to get GPU and main log files from the specified directory\n",
    "def get_gpu_and_main_log_files(dir_path: str) -> tuple:\n",
    "    import os\n",
    "    gpu_files = []  # List to store GPU idle files\n",
    "    gpu_util_files = []  # List to store GPU utilization files\n",
    "    log_files = {}  # Dictionary to store main and worker log files\n",
    "\n",
    "    # Walk through the directory and its subdirectories\n",
    "    for root, dirs, files in os.walk(dir_path):\n",
    "        log_files[root] = []  # Initialize the list for the current directory\n",
    "        for file in files:\n",
    "            # Check if the file is a GPU utilization file (excluding PNG files)\n",
    "            if \"gpu_util\" in file and 'png' not in file:\n",
    "                gpu_util_files.append(os.path.join(root, file))\n",
    "            # Check if the file is a GPU idle file (excluding PNG files)\n",
    "            if \"gpu_idle\" in file and 'png' not in file:\n",
    "                gpu_files.append(os.path.join(root, file))\n",
    "            # Check if the file is a main log file (excluding PNG files)\n",
    "            if \"main\" in file and 'png' not in file:\n",
    "                log_files[root].append(os.path.join(root, file))\n",
    "            # Check if the file is a worker log file (excluding PNG files)\n",
    "            if \"worker\" in file and 'png' not in file:\n",
    "                log_files[root].append(os.path.join(root, file))\n",
    "\n",
    "    # Return the lists of GPU idle files, main and worker log files, and GPU utilization files\n",
    "    return gpu_files, log_files, gpu_util_files\n",
    "\n",
    "# Get the GPU idle files, main and worker log files, and GPU utilization files from the specified directory\n",
    "gpu_files, log_files, gpu_util_files = get_gpu_and_main_log_files(dir_)\n",
    "\n",
    "# Filter out empty log file entries\n",
    "log_files = {key: val for key, val in log_files.items() if val}\n",
    "\n",
    "# Sort the log file keys naturally\n",
    "log_files_keys = natsort.natsorted(log_files)\n",
    "\n",
    "# Sort the GPU idle files naturally\n",
    "gpu_files = natsort.natsorted(gpu_files)\n",
    "\n",
    "# Sort the GPU utilization files naturally\n",
    "gpu_util_files = natsort.natsorted(gpu_util_files)\n",
    "\n",
    "\n",
    "for key in log_files_keys:\n",
    "    log_files[key] = natsort.natsorted(log_files[key])\n",
    "    print(key,log_files[key])\n",
    "print(gpu_files)\n",
    "print(len(log_files_keys), len(gpu_files), len(gpu_util_files))\n",
    "\n",
    "\n",
    "for key, gpu_file, gpu_util_file in zip(log_files_keys, gpu_files, gpu_util_files):\n",
    "    log_files_list = log_files[key]\n",
    "    # keep only the text after the last '/'\n",
    "    key = key.split('/')[-1]\n",
    "    print(key)\n",
    "    df_dict_everything_good_alloc_only[key] = get_everything(log_files_list, gpu_file, gpu_util_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gpu2 = {}\n",
    "df_gpu3 = {}\n",
    "df_gpu4 = {}\n",
    "\n",
    "for key in df_dict_everything:\n",
    "    if \"gpu2\" in key:\n",
    "        df_gpu2[key] = df_dict_everything[key]\n",
    "    if \"gpu3\" in key:\n",
    "        df_gpu3[key] = df_dict_everything[key]\n",
    "    if \"gpu4\" in key:\n",
    "        df_gpu4[key] = df_dict_everything[key]\n",
    "\n",
    "df_gpu2_io = {}\n",
    "df_gpu3_io = {}\n",
    "df_gpu4_io = {}\n",
    "\n",
    "for key in df_dict_everything_io_only:\n",
    "    if \"gpu2\" in key:\n",
    "        df_gpu2_io[key] = df_dict_everything_io_only[key]\n",
    "    if \"gpu3\" in key:\n",
    "        df_gpu3_io[key] = df_dict_everything_io_only[key]\n",
    "    if \"gpu4\" in key:\n",
    "        df_gpu4_io[key] = df_dict_everything_io_only[key]\n",
    "\n",
    "df_gpu2_good_alloc = {}\n",
    "df_gpu3_good_alloc = {}\n",
    "df_gpu4_good_alloc = {}\n",
    "for key in df_dict_everything_good_alloc_only:\n",
    "    if \"gpu2\" in key:\n",
    "        df_gpu2_good_alloc[key] = df_dict_everything_good_alloc_only[key]\n",
    "    if \"gpu3\" in key:\n",
    "        df_gpu3_good_alloc[key] = df_dict_everything_good_alloc_only[key]\n",
    "    if \"gpu4\" in key:\n",
    "        df_gpu4_good_alloc[key] = df_dict_everything_good_alloc_only[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_worker_log_files(dir_path: str) -> tuple:\n",
    "    import os\n",
    "    log_files = {}\n",
    "    for root, dirs, files in os.walk(dir_path):\n",
    "        log_files[root] = []\n",
    "        for file in files:\n",
    "            if \"worker\" in file and 'png' not in file:\n",
    "                # log_files.append(os.path.join(root, file))\n",
    "                log_files[root].append(os.path.join(root, file))\n",
    "    return log_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_ = \"../final_analysis_cloudlab_twenty\"\n",
    "\n",
    "log_files = get_worker_log_files(dir_)\n",
    "print(log_files)\n",
    "# For each key, process the log files\n",
    "df_worker = {}\n",
    "for key in log_files:\n",
    "    log_files_list = log_files[key]\n",
    "    for log_file in log_files_list:\n",
    "        if \"worker\" in log_file:\n",
    "            df_worker[log_file] = get_batch_preprocessing_times_with_ts(log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_ = \"../final_analysis_cloudlab_io_badalloc_twenty\"\n",
    "\n",
    "log_files = get_worker_log_files(dir_)\n",
    "print(log_files)\n",
    "# For each key, process the log files\n",
    "df_worker_io_only = {}\n",
    "for key in log_files:\n",
    "    log_files_list = log_files[key]\n",
    "    for log_file in log_files_list:\n",
    "        if \"worker\" in log_file:\n",
    "            df_worker_io_only[log_file] = get_batch_preprocessing_times_with_ts(log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_ = \"../final_analysis_cloudlab_good_alloc_twenty\"\n",
    "\n",
    "log_files = get_worker_log_files(dir_)\n",
    "print(log_files)\n",
    "# For each key, process the log files\n",
    "df_worker_goodalloc_only = {}\n",
    "for key in log_files:\n",
    "    log_files_list = log_files[key]\n",
    "    for log_file in log_files_list:\n",
    "        if \"worker\" in log_file:\n",
    "            df_worker_goodalloc_only[log_file] = get_batch_preprocessing_times_with_ts(log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_ = \"../final_analysis_cloudlab_io_twenty\"\n",
    "\n",
    "log_files = get_worker_log_files(dir_)\n",
    "print(log_files)\n",
    "# For each key, process the log files\n",
    "df_worker_io = {}\n",
    "for key in log_files:\n",
    "    log_files_list = log_files[key]\n",
    "    for log_file in log_files_list:\n",
    "        if \"worker\" in log_file:\n",
    "            df_worker_io[log_file] = get_batch_preprocessing_times_with_ts(log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each key, calculate the idle time as per the formula\n",
    "# idle_time = preprocessing_time_ts[cur_batch] - (preprocessing_time_ts[prev_batch] + preprocessing_time[prev_batch])\n",
    "def calculate_idle_time(df_worker):\n",
    "    for key in df_worker:\n",
    "        if 'gpu1' in key:\n",
    "            continue\n",
    "        if 'w32' in key:\n",
    "            continue\n",
    "        df = df_worker[key]\n",
    "        idle_times = []\n",
    "        for batch_id in df.index:\n",
    "            # find the next batch id in index\n",
    "            # it is not batch_id + 1 because the index is not sequential\n",
    "            # skip first 4 batches\n",
    "            # The batch id is not sequential\n",
    "            if batch_id < df.index[4]:\n",
    "                # add 0 to idle times\n",
    "                idle_times.append(0)\n",
    "                continue        \n",
    "            try:\n",
    "                next_batch_id = df.index[df.index.get_loc(batch_id) + 1]\n",
    "            except:\n",
    "                continue\n",
    "            if next_batch_id in df.index:\n",
    "                idle_time = df.loc[next_batch_id]['preprocessing_time_ts']/10**9 - (df.loc[batch_id]['preprocessing_time_ts']/10**9 + df.loc[batch_id]['preprocessing_time'])\n",
    "                idle_times.append(idle_time)\n",
    "        idle_times.append(0)\n",
    "        df_worker[key]['idle_time'] = idle_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_idle_time(df_worker)\n",
    "calculate_idle_time(df_worker_io_only)\n",
    "calculate_idle_time(df_worker_goodalloc_only)\n",
    "calculate_idle_time(df_worker_io)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_worker_gpu2 = {}\n",
    "for key in df_worker:\n",
    "    if \"gpu2\" in key:\n",
    "        df_worker_gpu2[key] = df_worker[key]\n",
    "df_worker_gpu2_io_only = {}\n",
    "for key in df_worker_io_only:\n",
    "    if \"gpu2\" in key:\n",
    "        df_worker_gpu2_io_only[key] = df_worker_io_only[key]\n",
    "df_worker_gpu3 = {}\n",
    "for key in df_worker:\n",
    "    if \"gpu3\" in key:\n",
    "        df_worker_gpu3[key] = df_worker[key]\n",
    "df_worker_gpu3_io_only = {}\n",
    "for key in df_worker_io_only:\n",
    "    if \"gpu3\" in key:\n",
    "        df_worker_gpu3_io_only[key] = df_worker_io_only[key]\n",
    "df_worker_gpu4 = {}\n",
    "for key in df_worker:\n",
    "    if \"gpu4\" in key:\n",
    "        df_worker_gpu4[key] = df_worker[key]\n",
    "df_worker_gpu4_io_only = {}\n",
    "for key in df_worker_io_only:\n",
    "    if \"gpu4\" in key:\n",
    "        df_worker_gpu4_io_only[key] = df_worker_io_only[key]\n",
    "\n",
    "df_worker_gpu2_good_alloc = {}\n",
    "for key in df_worker_goodalloc_only:\n",
    "    if \"gpu2\" in key:\n",
    "        df_worker_gpu2_good_alloc[key] = df_worker_goodalloc_only[key]\n",
    "df_worker_gpu3_good_alloc = {}\n",
    "for key in df_worker_goodalloc_only:\n",
    "    if \"gpu3\" in key:\n",
    "        df_worker_gpu3_good_alloc[key] = df_worker_goodalloc_only[key]\n",
    "df_worker_gpu4_good_alloc = {}\n",
    "for key in df_worker_goodalloc_only:\n",
    "    if \"gpu4\" in key:\n",
    "        df_worker_gpu4_good_alloc[key] = df_worker_goodalloc_only[key]\n",
    "\n",
    "        \n",
    "df_worker_gpu2_io = {}\n",
    "for key in df_worker_io:\n",
    "    if \"gpu2\" in key:\n",
    "        df_worker_gpu2_io[key] = df_worker_io[key]\n",
    "df_worker_gpu3_io = {}\n",
    "for key in df_worker_io:\n",
    "    if \"gpu3\" in key:\n",
    "        df_worker_gpu3_io[key] = df_worker_io[key]\n",
    "df_worker_gpu4_io = {}\n",
    "for key in df_worker_io:\n",
    "    if \"gpu4\" in key:\n",
    "        df_worker_gpu4_io[key] = df_worker_io[key]\n",
    "\n",
    "print(\"df_gpu2\",len(df_gpu2))\n",
    "print(\"df_gpu3\",len(df_gpu3))\n",
    "print(\"df_gpu4\",len(df_gpu4))\n",
    "print(\"df_gpu2_io\",len(df_gpu2_io))\n",
    "print(\"df_gpu3_io\",len(df_gpu3_io))\n",
    "print(\"df_gpu4_io\",len(df_gpu4_io))\n",
    "print(\"df_gpu2_good_alloc\",len(df_gpu2_good_alloc))\n",
    "print(\"df_gpu3_good_alloc\",len(df_gpu3_good_alloc))\n",
    "print(\"df_gpu4_good_alloc\",len(df_gpu4_good_alloc))\n",
    "print(\"df_worker_gpu2\",len(df_worker_gpu2))\n",
    "print(\"df_worker_gpu3\",len(df_worker_gpu3))\n",
    "print(\"df_worker_gpu4\",len(df_worker_gpu4))\n",
    "print(\"df_worker_gpu2_io\",len(df_worker_gpu2_io))\n",
    "print(\"df_worker_gpu3_io\",len(df_worker_gpu3_io))\n",
    "print(\"df_worker_gpu4_io\",len(df_worker_gpu4_io))\n",
    "print(\"df_worker_gpu2_good_alloc\",len(df_worker_gpu2_good_alloc))\n",
    "print(\"df_worker_gpu3_good_alloc\",len(df_worker_gpu3_good_alloc))\n",
    "print(\"df_worker_gpu4_good_alloc\",len(df_worker_gpu4_good_alloc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from natsort import natsorted\n",
    "\n",
    "def plot_total_idle_time(df_worker):\n",
    "    # Extract configuration from keys\n",
    "    config_idle_times = {}\n",
    "    for key, df in df_worker.items():\n",
    "        # config = key.split('/')[2]  # Extract 'b128_gpu1_w24' part\n",
    "        config = key.split('/')[2]\n",
    "        if config not in config_idle_times:\n",
    "            config_idle_times[config] = []\n",
    "        config_idle_times[config].append(df['idle_time'].sum())\n",
    "\n",
    "    # Calculate total idle time for each configuration\n",
    "    total_idle_times = {config: np.sum(times) for config, times in config_idle_times.items()}\n",
    "\n",
    "    # Create a DataFrame for plotting\n",
    "    plot_df = pd.DataFrame.from_dict(total_idle_times, orient='index', columns=['Total Idle Time'])\n",
    "    plot_df['Configuration'] = plot_df.index\n",
    "    plot_df['Batch Size'] = plot_df['Configuration'].apply(lambda x: x.split('_')[0][1:])\n",
    "    plot_df['GPUs'] = plot_df['Configuration'].apply(lambda x: x.split('_')[1][3:])\n",
    "    plot_df['Num Workers'] = plot_df['Configuration'].apply(lambda x: x.split('_')[2][1:])\n",
    "\n",
    "    # Sort the DataFrame using natural sort for configurations\n",
    "    plot_df = plot_df.loc[natsorted(plot_df.index)]\n",
    "\n",
    "    # Set up the plot style\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    plt.figure(figsize=(20, 12), dpi=300)\n",
    "\n",
    "    # Create the bar plot\n",
    "    ax = sns.barplot(x='Configuration', y='Total Idle Time', data=plot_df, palette='viridis')\n",
    "\n",
    "    # Customize the plot\n",
    "    plt.title('Total Worker Idle Time per Configuration', fontsize=20, pad=20)\n",
    "    plt.xlabel('Experiment Configuration', fontsize=16, labelpad=10)\n",
    "    plt.ylabel('Total Idle Time (seconds)', fontsize=16, labelpad=10)\n",
    "    plt.xticks(rotation=45, ha='right', fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "\n",
    "    # Add value labels on top of each bar\n",
    "    for i, v in enumerate(plot_df['Total Idle Time']):\n",
    "        ax.text(i, v, f'{v:.2f}', ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "\n",
    "    # Remove top and right spines\n",
    "    sns.despine()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('total_idle_time_bar_plot.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # Create a heatmap\n",
    "    # Group by 'Batch Size', 'GPUs', and 'Num Workers', and take the mean of 'Total Idle Time'\n",
    "    grouped_df = plot_df.groupby(['Batch Size', 'GPUs', 'Num Workers'])['Total Idle Time'].mean().reset_index()\n",
    "    \n",
    "    # Sort the grouped DataFrame using natural sorting for Batch Size and GPUs/Num Workers\n",
    "    grouped_df['Batch Size'] = pd.to_numeric(grouped_df['Batch Size'], errors='coerce')  # Ensure Batch Size is numeric for sorting\n",
    "    grouped_df.sort_values(by=['Batch Size', 'GPUs', 'Num Workers'], inplace=True)\n",
    "\n",
    "    pivot_df = grouped_df.pivot(index='Batch Size', columns=['GPUs', 'Num Workers'], values='Total Idle Time')\n",
    "\n",
    "    # Ensure natural sorting of columns in pivot table\n",
    "    pivot_df.columns = natsorted(pivot_df.columns)\n",
    "\n",
    "    plt.figure(figsize=(16, 10), dpi=300)\n",
    "    sns.heatmap(pivot_df, annot=True, fmt='.2f', cmap='YlOrRd', annot_kws={'size': 10})\n",
    "    \n",
    "    plt.title('Total Worker Idle Time: Batch Size vs (GPUs, Num Workers)', fontsize=20)\n",
    "    plt.xlabel('(GPUs, Num Workers)', fontsize=16)\n",
    "    plt.ylabel('Batch Size', fontsize=16)\n",
    "    \n",
    "    plt.xticks(fontsize=12, rotation=45, ha='right')\n",
    "    plt.yticks(fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('total_idle_time_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_total_idle_time(df_worker_gpu2)\n",
    "# plot_total_idle_time(df_worker_gpu3)\n",
    "# plot_total_idle_time(df_worker_gpu4)\n",
    "# plot_total_idle_time(df_worker_gpu2_io)\n",
    "# plot_total_idle_time(df_worker_gpu3_io)\n",
    "# plot_total_idle_time(df_worker_gpu4_io)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_e2e_times(dir_path: str) -> list:\n",
    "    import os\n",
    "    e2e_times = {}\n",
    "    for root, dirs, files in os.walk(dir_path):\n",
    "        for file in files:\n",
    "            if \"lotustrace_log\" in file:\n",
    "                # custom_log_b128_gpu1\n",
    "                key = file.replace(\"lotustrace_log_\",\"\").replace(\".log\",\"\")\n",
    "                # key = file.split('_')[-2] + \"_\" + file.split('_')[-1]\n",
    "                # file is a csv\n",
    "                df = pd.read_csv(os.path.join(root, file))\n",
    "                try:\n",
    "                    e2e_times[key] = df['wall(s)'].abs().sum()\n",
    "                except:\n",
    "                    e2e_times[key] = 0\n",
    "    return e2e_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_1 = \"../final_analysis_cloudlab_twenty/e2e\"\n",
    "dir_2 = \"../final_analysis_cloudlab_io_badalloc_twenty/e2e\"\n",
    "dir_3 = \"../final_analysis_cloudlab_good_alloc_twenty/e2e\"\n",
    "dir_4 = \"../final_analysis_cloudlab_io_twenty/e2e\"\n",
    "\n",
    "e2e_times_1 = get_e2e_times(dir_1)\n",
    "e2e_times_2 = get_e2e_times(dir_2)\n",
    "e2e_times_3 = get_e2e_times(dir_3)\n",
    "e2e_times_4 = get_e2e_times(dir_4)\n",
    "\n",
    "# sort using natsort\n",
    "e2e_times_1 = dict(sorted(e2e_times_1.items(), key=lambda x: natsort.natsort_key(x[0].lower())))\n",
    "e2e_times_2 = dict(sorted(e2e_times_2.items(), key=lambda x: natsort.natsort_key(x[0].lower())))\n",
    "e2e_times_3 = dict(sorted(e2e_times_3.items(), key=lambda x: natsort.natsort_key(x[0].lower())))\n",
    "e2e_times_4 = dict(sorted(e2e_times_4.items(), key=lambda x: natsort.natsort_key(x[0].lower())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import natsort\n",
    "from natsort import natsorted\n",
    "\n",
    "def plot_total_idle_time_side_by_side(df_worker, df_worker_io):\n",
    "    # Compute idle times for df_worker\n",
    "    config_idle_times_worker = {}\n",
    "    for key, df in df_worker.items():\n",
    "        # Extract configuration string, e.g., 'b128_gpu1_w24'\n",
    "        config = key.split('/')[2]\n",
    "        # config = key\n",
    "        config_idle_times_worker.setdefault(config, []).append(df['idle_time'].sum())\n",
    "    total_idle_times_worker = {config: np.sum(times) for config, times in config_idle_times_worker.items()}\n",
    "    # apply the formula total worker idle/(total E2E * num of cpu)\n",
    "    total_idle_times_worker = {config: total_idle_times_worker[config]/(e2e_times_1[config] * int(config.split('_')[2][1:])) for config in total_idle_times_worker}\n",
    "    # print(total_idle_times_worker)\n",
    "    # Compute idle times for df_worker_io\n",
    "    config_idle_times_worker_io = {}\n",
    "    for key, df in df_worker_io.items():\n",
    "        config = key.split('/')[2]\n",
    "        # config = key\n",
    "        config_idle_times_worker_io.setdefault(config, []).append(df['idle_time'].sum())\n",
    "    total_idle_times_worker_io = {config: np.sum(times) for config, times in config_idle_times_worker_io.items()}\n",
    "    # apply the formula total worker idle/(total E2E * num of cpu)\n",
    "    total_idle_times_worker_io = {config: total_idle_times_worker_io[config]/(e2e_times_2[config] * int(config.split('_')[2][1:])) for config in total_idle_times_worker_io}\n",
    "\n",
    "    # Use the union of keys from both dictionaries and sort naturally\n",
    "    all_configs = natsorted(set(total_idle_times_worker.keys()).union(set(total_idle_times_worker_io.keys())))\n",
    "\n",
    "    # Create a combined DataFrame with both idle time values for each configuration\n",
    "    data = []\n",
    "    for config in all_configs:\n",
    "        worker_idle = total_idle_times_worker.get(config, 0)\n",
    "        worker_io_idle = total_idle_times_worker_io.get(config, 0)\n",
    "        # Extract additional details from configuration\n",
    "        batch_size = config.split('_')[0][1:]\n",
    "        gpus = config.split('_')[1][3:]\n",
    "        num_workers = config.split('_')[2][1:]\n",
    "        data.append({\n",
    "            'Configuration': config,\n",
    "            'Worker Inefficieny': worker_idle,\n",
    "            'Worker Optimised Inefficieny': worker_io_idle,\n",
    "            'Batch Size': batch_size,\n",
    "            'GPUs': gpus,\n",
    "            'Num Workers': num_workers\n",
    "        })\n",
    "\n",
    "    plot_df = pd.DataFrame(data)\n",
    "\n",
    "    # Melt the DataFrame for side-by-side bar visualization using seaborn\n",
    "    plot_df_melt = plot_df.melt(id_vars=['Configuration', 'Batch Size', 'GPUs', 'Num Workers'],\n",
    "                                value_vars=['Worker Inefficieny', 'Worker Optimised Inefficieny'],\n",
    "                                var_name='Task Type', value_name='Idle Time')\n",
    "\n",
    "    # ----------------------- Bar Plot -----------------------\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    plt.figure(figsize=(20, 12), dpi=300)\n",
    "    ax = sns.barplot(x='Configuration', y='Idle Time', hue='Task Type', data=plot_df_melt, palette='viridis')\n",
    "\n",
    "    plt.title('Inefficieny per Configuration: Original vs Optimised', fontsize=20, pad=20)\n",
    "    plt.xlabel('Experiment Configuration', fontsize=16, labelpad=10)\n",
    "    plt.ylabel('Inefficieny', fontsize=16, labelpad=10)\n",
    "    plt.xticks(rotation=45, ha='right', fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "\n",
    "    # Add value labels above each bar\n",
    "    for p in ax.patches:\n",
    "        height = p.get_height()\n",
    "        # The x coordinate is obtained from the bar's x location and width\n",
    "        ax.annotate(f'{height:.2f}',\n",
    "                    (p.get_x() + p.get_width() / 2, height),\n",
    "                    ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "    sns.despine()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('idle_time_side_by_side_bar_plot.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Example call (assuming df_worker and df_worker_io dictionaries are defined)\n",
    "plot_total_idle_time_side_by_side(df_worker_gpu2, df_worker_gpu2_io)\n",
    "plot_total_idle_time_side_by_side(df_worker_gpu3, df_worker_gpu3_io)\n",
    "plot_total_idle_time_side_by_side(df_worker_gpu4, df_worker_gpu4_io)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from natsort import natsorted\n",
    "\n",
    "def plot_total_idle_time_side_by_side(df_worker, df_worker_io_only, df_worker_goodalloc, df_worker_io, e2e_times_1, e2e_times_2, e2e_times_3, e2e_times_4):\n",
    "    # Compute idle times for df_worker\n",
    "    config_idle_times_worker = {}\n",
    "    for key, df in df_worker.items():\n",
    "        if 'b256' in key:\n",
    "            continue\n",
    "        if 'b1024' in key:\n",
    "            continue\n",
    "        config = key.split('/')[2]\n",
    "        config_idle_times_worker.setdefault(config, []).append(df['idle_time'].sum())\n",
    "    total_idle_times_worker = {config: np.sum(times) for config, times in config_idle_times_worker.items()}\n",
    "    total_idle_times_worker = {\n",
    "        config: total_idle_times_worker[config] / (e2e_times_1[config] * int(config.split('_')[2][1:]))\n",
    "        for config in total_idle_times_worker\n",
    "    }\n",
    "\n",
    "    # Compute idle times for df_worker_io_only\n",
    "    config_idle_times_worker_io = {}\n",
    "    for key, df in df_worker_io_only.items():\n",
    "        if 'b256' in key:\n",
    "            continue\n",
    "        if 'b1024' in key:\n",
    "            continue\n",
    "        config = key.split('/')[2]\n",
    "        config_idle_times_worker_io.setdefault(config, []).append(df['idle_time'].sum())\n",
    "    total_idle_times_worker_io_only = {config: np.sum(times) for config, times in config_idle_times_worker_io.items()}\n",
    "    total_idle_times_worker_io_only = {\n",
    "        config: total_idle_times_worker_io_only[config] / (e2e_times_2[config] * int(config.split('_')[2][1:]))\n",
    "        for config in total_idle_times_worker_io_only\n",
    "    }\n",
    "\n",
    "    # Compute idle times for df_worker_goodalloc\n",
    "    config_idle_times_worker_goodalloc = {}\n",
    "    for key, df in df_worker_goodalloc.items():\n",
    "        if 'b256' in key:\n",
    "            continue\n",
    "        if 'b1024' in key:\n",
    "            continue\n",
    "        if 'w32' in key:\n",
    "            continue\n",
    "        config = key.split('/')[2]\n",
    "        config_idle_times_worker_goodalloc.setdefault(config, []).append(df['idle_time'].sum())\n",
    "    total_idle_times_worker_goodalloc = {config: np.sum(times) for config, times in config_idle_times_worker_goodalloc.items()}\n",
    "    total_idle_times_worker_goodalloc = {\n",
    "        config: total_idle_times_worker_goodalloc[config] / (e2e_times_3[config] * int(config.split('_')[2][1:]))\n",
    "        for config in total_idle_times_worker_goodalloc\n",
    "    }\n",
    "\n",
    "\n",
    "    # Compute idle times for df_worker_io\n",
    "    config_idle_times_worker_io = {}\n",
    "    for key, df in df_worker_io.items():\n",
    "        if 'b256' in key:\n",
    "            continue\n",
    "        if 'b1024' in key:\n",
    "            continue\n",
    "        config = key.split('/')[2]\n",
    "        config_idle_times_worker_io.setdefault(config, []).append(df['idle_time'].sum())\n",
    "    total_idle_times_worker_io = {config: np.sum(times) for config, times in config_idle_times_worker_io.items()}\n",
    "    total_idle_times_worker_io = {\n",
    "        config: total_idle_times_worker_io[config] / (e2e_times_4[config] * int(config.split('_')[2][1:]))\n",
    "        for config in total_idle_times_worker_io\n",
    "    }\n",
    "\n",
    "    # Use the union of keys from all dictionaries and sort naturally\n",
    "    all_configs = natsorted(\n",
    "        set(total_idle_times_worker.keys())\n",
    "        | set(total_idle_times_worker_io.keys())\n",
    "        | set(total_idle_times_worker_goodalloc.keys())\n",
    "        | set(total_idle_times_worker_io.keys())\n",
    "    )\n",
    "    # Create a combined DataFrame with all idle time values for each configuration\n",
    "    data = []\n",
    "    for config in all_configs:\n",
    "        worker_idle = total_idle_times_worker.get(config, 0)\n",
    "        worker_io_only_idle = total_idle_times_worker_io_only.get(config, 0)\n",
    "        worker_goodalloc_idle = total_idle_times_worker_goodalloc.get(config, 0)\n",
    "        worker_io_idle = total_idle_times_worker_io.get(config, 0)\n",
    "        batch_size = config.split('_')[0][1:]\n",
    "        gpus = config.split('_')[1][3:]\n",
    "        num_workers = config.split('_')[2][1:]\n",
    "        data.append({\n",
    "            'Configuration': config,\n",
    "            'Original Pipeline': worker_idle,\n",
    "            'Optimised Worker Assignment': worker_goodalloc_idle,\n",
    "            'Optimised Batch Pinning': worker_io_only_idle,\n",
    "            'Optimised Worker Assignment + Batch Pinning': worker_io_idle,\n",
    "            'Batch Size': batch_size,\n",
    "            'GPUs': gpus,\n",
    "            'Num Workers': num_workers\n",
    "        })\n",
    "\n",
    "    plot_df = pd.DataFrame(data)\n",
    "\n",
    "    transition_index = next((i for i, k in enumerate(all_configs) if 'b512' in k), None)\n",
    "\n",
    "    # Melt the DataFrame for side-by-side bar visualization using seaborn\n",
    "    plot_df_melt = plot_df.melt(\n",
    "        id_vars=['Configuration', 'Batch Size', 'GPUs', 'Num Workers'],\n",
    "        # value_vars=['Original Pipeline', 'IO Only Pipeline', 'Good Allocation Pipeline'],\n",
    "        value_vars=['Original Pipeline', 'Optimised Worker Assignment', 'Optimised Batch Pinning', 'Optimised Worker Assignment + Batch Pinning'],\n",
    "        var_name='Pipeline Type', value_name='Idle Time'\n",
    "    )\n",
    "\n",
    "    # ----------------------- Bar Plot -----------------------\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    plt.figure(figsize=(20, 12), dpi=1200)\n",
    "    ax = sns.barplot(\n",
    "        x='Configuration', y='Idle Time', hue='Pipeline Type',\n",
    "        data=plot_df_melt, palette='viridis'\n",
    "    )\n",
    "\n",
    "    plt.title('Worker Inefficiency per Configuration: Original vs Optimised Worker Assignment vs Optimised Batch Pinning vs Optimised Worker Assignment + Batch Pinning', fontsize=20, pad=20)\n",
    "    plt.xlabel('Experiment Configuration', fontsize=16, labelpad=10)\n",
    "    plt.ylabel('Normalized Worker Idle Time', fontsize=16, labelpad=10)\n",
    "    plt.xticks(rotation=45, ha='right', fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "\n",
    "    # Add value labels above each bar\n",
    "    for p in ax.patches:\n",
    "        height = p.get_height()\n",
    "        ax.annotate(f'{height:.2f}',\n",
    "                    (p.get_x() + p.get_width() / 2, height),\n",
    "                    ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "    ax.axvline(x=transition_index-0.5, color='black', linestyle='--', linewidth=2)\n",
    "\n",
    "    sns.despine()\n",
    "    # plt.tight_layout()\n",
    "    # plt.savefig('idle_time_side_by_side_bar_plot.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "# Example call (assuming df_worker, df_worker_io, and df_worker_goodalloc dictionaries are defined)\n",
    "# plot_total_idle_time_side_by_side(df_worker_gpu2, df_worker_gpu2_io, df_worker_gpu2_good_alloc, e2e_times_1, e2e_times_2, e2e_times_3)\n",
    "plot_total_idle_time_side_by_side(df_worker_gpu3, df_worker_gpu3_io_only, df_worker_gpu3_good_alloc, df_worker_gpu3_io, e2e_times_1, e2e_times_2, e2e_times_3, e2e_times_4)\n",
    "# plot_total_idle_time_side_by_side(df_worker_gpu4, df_worker_gpu4_io, df_worker_gpu4_good_alloc, e2e_times_1, e2e_times_2, e2e_times_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sprob",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
