{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import json,argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# plot the data\n",
    "import matplotlib.pyplot as plt\n",
    "import natsort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the directory to search for log files\n",
    "dir_ = \"../final_analysis_cloudlab_twenty\"\n",
    "\n",
    "# Function to get GPU and main log files from the specified directory\n",
    "def get_gpu_and_main_log_files(dir_path: str) -> tuple:\n",
    "    import os\n",
    "    gpu_files = []  # List to store GPU idle files\n",
    "    gpu_util_files = []  # List to store GPU utilization files\n",
    "    log_files = {}  # Dictionary to store main and worker log files\n",
    "\n",
    "    # Walk through the directory and its subdirectories\n",
    "    for root, dirs, files in os.walk(dir_path):\n",
    "        log_files[root] = []  # Initialize the list for the current directory\n",
    "        for file in files:\n",
    "            # Check if the file is a GPU utilization file (excluding PNG files)\n",
    "            if \"gpu_util\" in file and 'png' not in file:\n",
    "                gpu_util_files.append(os.path.join(root, file))\n",
    "            # Check if the file is a GPU idle file (excluding PNG files)\n",
    "            if \"gpu_idle\" in file and 'png' not in file:\n",
    "                gpu_files.append(os.path.join(root, file))\n",
    "            # Check if the file is a main log file (excluding PNG files)\n",
    "            if \"main\" in file and 'png' not in file:\n",
    "                log_files[root].append(os.path.join(root, file))\n",
    "            # Check if the file is a worker log file (excluding PNG files)\n",
    "            if \"worker\" in file and 'png' not in file:\n",
    "                log_files[root].append(os.path.join(root, file))\n",
    "\n",
    "    # Return the lists of GPU idle files, main and worker log files, and GPU utilization files\n",
    "    return gpu_files, log_files, gpu_util_files\n",
    "\n",
    "# Get the GPU idle files, main and worker log files, and GPU utilization files from the specified directory\n",
    "gpu_files, log_files, gpu_util_files = get_gpu_and_main_log_files(dir_)\n",
    "\n",
    "# Filter out empty log file entries\n",
    "log_files = {key: val for key, val in log_files.items() if val}\n",
    "\n",
    "# Sort the log file keys naturally\n",
    "log_files_keys = natsort.natsorted(log_files)\n",
    "\n",
    "# Sort the GPU idle files naturally\n",
    "gpu_files = natsort.natsorted(gpu_files)\n",
    "\n",
    "# Sort the GPU utilization files naturally\n",
    "gpu_util_files = natsort.natsorted(gpu_util_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in log_files_keys:\n",
    "    log_files[key] = natsort.natsorted(log_files[key])\n",
    "    print(key,log_files[key])\n",
    "print(gpu_files)\n",
    "print(len(log_files_keys), len(gpu_files), len(gpu_util_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict_everything = {}\n",
    "\n",
    "def get_batch_pin_memory_time_with_ts(log_file: str) -> pd.DataFrame:\n",
    "    with open(log_file) as f:\n",
    "        lines = f.readlines()\n",
    "        batch_pin_memory_times = {}\n",
    "        batch_pin_memory_times_ts = {}\n",
    "        \n",
    "        for line in lines:\n",
    "            if \"SBatchPinMemory\" in line:\n",
    "                parts = line.split(',')\n",
    "                batch_id = int(parts[0].split('_')[1])\n",
    "                pin_memory_time = float(parts[-1]) / (1000 * 1000 * 1000)\n",
    "                batch_pin_memory_times[batch_id] = pin_memory_time\n",
    "                batch_pin_memory_times_ts[batch_id] = float(parts[-2])\n",
    "        \n",
    "        data = {\n",
    "            'batch_id': list(batch_pin_memory_times.keys()),\n",
    "            'pin_memory_time': list(batch_pin_memory_times.values()),\n",
    "            'pin_memory_time_ts': list(batch_pin_memory_times_ts.values())\n",
    "        }\n",
    "        \n",
    "        df = pd.DataFrame(data)\n",
    "        df.set_index('batch_id', inplace=True)\n",
    "        df.sort_index(inplace=True)\n",
    "        \n",
    "        return df\n",
    "\n",
    "def get_batch_idle_times_with_ts(log_file: str) -> pd.DataFrame:\n",
    "    with open(log_file) as f:\n",
    "        lines = f.readlines()\n",
    "        batch_wait_times = {}\n",
    "        batch_wait_times_ts = {}\n",
    "        \n",
    "        for line in lines:\n",
    "            if \"SBatchWait\" in line:\n",
    "                parts = line.split(',')\n",
    "                batch_id = int(parts[0].split('_')[1])\n",
    "                batch_wait_times[batch_id] = float(parts[2]) / (1000 * 1000 * 1000)\n",
    "                batch_wait_times_ts[batch_id] = float(parts[1])\n",
    "        \n",
    "        data = {\n",
    "            'batch_id': list(batch_wait_times.keys()),\n",
    "            'wait_time': list(batch_wait_times.values()),\n",
    "            'wait_time_ts': list(batch_wait_times_ts.values())\n",
    "        }\n",
    "        \n",
    "        df = pd.DataFrame(data)\n",
    "        df.set_index('batch_id', inplace=True)\n",
    "        df.sort_index(inplace=True)\n",
    "        \n",
    "        return df\n",
    "\n",
    "def get_batch_preprocessing_times_with_ts(log_file: str) -> pd.DataFrame:\n",
    "    with open(log_file) as f:\n",
    "        lines = f.readlines()\n",
    "        batch_preprocessing_times = {}\n",
    "        batch_preprocessing_times_ts = {}\n",
    "        \n",
    "        for line in lines:\n",
    "            if \"SBatchPreprocessed\" in line:\n",
    "                parts = line.split(',')\n",
    "                batch_id = int(parts[0].split('_')[1])\n",
    "                preprocessing_time = float(parts[-1]) / (1000 * 1000 * 1000)\n",
    "                batch_preprocessing_times[batch_id] = preprocessing_time\n",
    "                batch_preprocessing_times_ts[batch_id] = float(parts[-2])\n",
    "        \n",
    "        data = {\n",
    "            'batch_id': list(batch_preprocessing_times.keys()),\n",
    "            'preprocessing_time': list(batch_preprocessing_times.values()),\n",
    "            'preprocessing_time_ts': list(batch_preprocessing_times_ts.values())\n",
    "        }\n",
    "        \n",
    "        df = pd.DataFrame(data)\n",
    "        df.set_index('batch_id', inplace=True)\n",
    "        df.sort_index(inplace=True)\n",
    "        \n",
    "        return df\n",
    "\n",
    "def get_batch_consumed_times_with_ts(log_file: str) -> pd.DataFrame:\n",
    "    with open(log_file) as f:\n",
    "        lines = f.readlines()\n",
    "    batch_consumed_times = {}\n",
    "    batch_consumed_times_ts = {}\n",
    "    for line in lines:\n",
    "        if \"SBatchConsumed\" in line:\n",
    "            parts = line.split(',')\n",
    "            batch_id = int(parts[0].split('_')[1])\n",
    "            consumed_time = float(parts[-1]) / (1000 * 1000 * 1000)\n",
    "            batch_consumed_times[batch_id] = consumed_time\n",
    "            batch_consumed_times_ts[batch_id] = int(parts[-2])\n",
    "    \n",
    "    data = {\n",
    "        'batch_id': list(batch_consumed_times.keys()),\n",
    "        'consumed_time': list(batch_consumed_times.values()),\n",
    "        'consumed_time_ts': list(batch_consumed_times_ts.values())\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    df.set_index('batch_id', inplace=True)\n",
    "    df.sort_index(inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_gpu_util_time(gpu_util_file: str) -> pd.DataFrame:\n",
    "    util_times = []\n",
    "    batch_id = 1\n",
    "\n",
    "    with open(gpu_util_file) as f:\n",
    "        for line in f:\n",
    "            if \"ms\" in line:\n",
    "                util_time = float(line.split()[0]) / 1000\n",
    "                util_times.append((batch_id, util_time))\n",
    "                batch_id += 1\n",
    "\n",
    "    df = pd.DataFrame(util_times, columns=['batch_id', 'util_time'])\n",
    "    df.set_index('batch_id', inplace=True)\n",
    "    df.sort_index(inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_gpu_wait_time(gpu_file: str) -> pd.DataFrame:\n",
    "    idle_times = []\n",
    "    batch_id = 1\n",
    "    \n",
    "    with open(gpu_file) as f:\n",
    "        for line in f:\n",
    "            if \"ms\" in line:\n",
    "                idle_time = float(line.split()[0]) / 1000\n",
    "                idle_times.append((batch_id, idle_time))\n",
    "                batch_id += 1\n",
    "    \n",
    "    df = pd.DataFrame(idle_times, columns=['batch_id', 'idle_time'])\n",
    "    df.set_index('batch_id', inplace=True)\n",
    "    df.sort_index(inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_everything(log_files, gpu_file, gpu_util_file):\n",
    "    # get e2e time\n",
    "    df_main = None\n",
    "    for file in log_files:\n",
    "        if \"main\" in file:\n",
    "            # get the batch wait time by ID\n",
    "            df_batch_wait_times = get_batch_idle_times_with_ts(file)\n",
    "            # concat with df_main using batch_id\n",
    "            if df_main is None:\n",
    "                df_main = df_batch_wait_times\n",
    "            else:\n",
    "                df_main = df_main.combine_first(df_batch_wait_times)\n",
    "\n",
    "            df_batch_consumed_times = get_batch_consumed_times_with_ts(file)\n",
    "            # concat with df_main using batch_id\n",
    "            if df_main is None:\n",
    "                df_main = df_batch_consumed_times\n",
    "            else:\n",
    "                df_main = df_main.combine_first(df_batch_consumed_times)\n",
    "            \n",
    "            df_batch_pin_memory_times = get_batch_pin_memory_time_with_ts(file)\n",
    "            # concat with df_main using batch_id\n",
    "            if df_main is None:\n",
    "                df_main = df_batch_pin_memory_times\n",
    "            else:\n",
    "                df_main = df_main.combine_first(df_batch_pin_memory_times)\n",
    "            \n",
    "        if \"worker\" in file:\n",
    "            # get batch preprocessing time by ID\n",
    "            df_batch_preprocessing_times = get_batch_preprocessing_times_with_ts(file)\n",
    "            # concat with df_main using batch_id\n",
    "            if df_main is None:\n",
    "                df_main = df_batch_preprocessing_times\n",
    "            else:\n",
    "                df_main = df_main.combine_first(df_batch_preprocessing_times)\n",
    "    # in df_main, calculate wait_time_ts - (preprocessing_time_ts + preprocessing_time) for each batch and store in a new column\n",
    "    df_main['wait_time_preprocessing_time_ts_diff'] = df_main['wait_time_ts']/(1000 * 1000 * 1000) - (df_main['preprocessing_time_ts']/(1000 * 1000 * 1000) + df_main['preprocessing_time'])\n",
    "    df_main['wait_time_preprocessing_time_ts_diff'] = df_main['wait_time_preprocessing_time_ts_diff']\n",
    "    # in df_main, calculate (consumed_time_ts - (preprocessing_time_ts + preprocessing_time) prefor each batch and store in a new column\n",
    "    df_main['consumed_time_preprocessing_time_ts_diff'] = df_main['consumed_time_ts']/(1000 * 1000 * 1000) - (df_main['preprocessing_time_ts']/(1000 * 1000 * 1000) + df_main['preprocessing_time'])\n",
    "    df_main['consumed_time_preprocessing_time_ts_diff'] = df_main['consumed_time_preprocessing_time_ts_diff']\n",
    "    # get the idle times\n",
    "    df_gpu = get_gpu_wait_time(gpu_file)\n",
    "    df_gpu_util = get_gpu_util_time(gpu_util_file)\n",
    "    # concat with df_main using batch_id\n",
    "    # df_main = pd.concat([df_main,df_gpu],axis=1)\n",
    "    df_main = df_main.combine_first(df_gpu)\n",
    "    df_main = df_main.combine_first(df_gpu_util)\n",
    "    return df_main\n",
    "\n",
    "for key, gpu_file, gpu_util_file in zip(log_files_keys, gpu_files, gpu_util_files):\n",
    "    log_files_list = log_files[key]\n",
    "    # keep only the text after the last '/'\n",
    "    key = key.split('/')[-1]\n",
    "    print(key)\n",
    "    df_dict_everything[key] = get_everything(log_files_list, gpu_file, gpu_util_file)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def find_bad_overlapping_batches(df):\n",
    "    # Convert all timestamps to numpy arrays for faster operations\n",
    "    pin_times = df['pin_memory_time_ts'].values\n",
    "    wait_times = df['wait_time_ts'].values\n",
    "    preprocessing_times = df['preprocessing_time_ts'].values\n",
    "    preprocessing_duration = df['preprocessing_time'].values * 1e9  # Convert to nanoseconds\n",
    "    consumed_times = df['consumed_time_ts'].values\n",
    "    wait_duration = df['wait_time'].values * 1e9  # Convert to nanoseconds\n",
    "\n",
    "    \n",
    "    # Calculate the end time of waiting for each batch\n",
    "    wait_end_times = wait_times + wait_duration\n",
    "    preprocessing_end_times = preprocessing_times + preprocessing_duration\n",
    "    \n",
    "    # Create arrays for batch indices\n",
    "    batch_indices = np.arange(len(df))\n",
    "    \n",
    "    overlapping_pairs = []\n",
    "    \n",
    "    # Vectorized comparison for each batch\n",
    "    for i in batch_indices:\n",
    "        comparison_time = np.where(wait_times > preprocessing_end_times, wait_times, preprocessing_end_times)\n",
    "        mask = (\n",
    "            (pin_times[i] > comparison_time) &\n",
    "            (pin_times[i] < wait_end_times) &\n",
    "            (batch_indices != i)\n",
    "        )\n",
    "        \n",
    "        # Get the matching batch indices\n",
    "        matching_indices = batch_indices[mask]\n",
    "        \n",
    "        # Add all matching pairs\n",
    "        for j in matching_indices:\n",
    "            overlapping_pairs.append((i, j))\n",
    "    \n",
    "    return overlapping_pairs\n",
    "\n",
    "\n",
    "\n",
    "def find_ooo_batches(df):\n",
    "    ooo_batches = df[df['wait_time'] == 1e-6]\n",
    "    return ooo_batches\n",
    "\n",
    "def check_if_all_bad_overlapping_batches_are_ooo(df):\n",
    "    bad_overlapping_batches = find_bad_overlapping_batches(df)\n",
    "    ooo_batches = find_ooo_batches(df)\n",
    "    bad_overlapping_batches = set([pair[0] for pair in bad_overlapping_batches])\n",
    "    ooo_batches = set(ooo_batches.index)\n",
    "    # if assert fails, print the bad overlapping batches that are not OOO\n",
    "    if not bad_overlapping_batches.issubset(ooo_batches):\n",
    "        print(\"Bad overlapping batches that are not OOO:\", bad_overlapping_batches - ooo_batches)\n",
    "\n",
    "\n",
    "def find_good_overlapping_batches(df):\n",
    "    ooo_batches = find_ooo_batches(df)\n",
    "    overlapping_pairs = find_bad_overlapping_batches(df)\n",
    "    bad_overlapping_batches = set([pair[0] for pair in overlapping_pairs])\n",
    "    # good_overlapping_batches = ooo_batches.drop(bad_overlapping_batches)\n",
    "    # some bad overlapping batches are not in OOO, so drop whatever possible\n",
    "    good_overlapping_batches = ooo_batches[~ooo_batches.index.isin(bad_overlapping_batches)]\n",
    "    return good_overlapping_batches\n",
    "        \n",
    "\n",
    "\n",
    "# print total number of batches, number of overlapping batches, number of good overlapping batches, number of bad overlapping batches\n",
    "for key in df_dict_everything:\n",
    "    df = df_dict_everything[key]\n",
    "    print(key)\n",
    "    check_if_all_bad_overlapping_batches_are_ooo(df)\n",
    "    print(\"Total number of batches:\", len(df))\n",
    "    bad_overlapping_batches = find_bad_overlapping_batches(df)\n",
    "    bad_overlapping_batches = set([pair[0] for pair in bad_overlapping_batches])\n",
    "    good_overlapping_batches = find_good_overlapping_batches(df)\n",
    "    good_overlapping_batches = set(good_overlapping_batches.index)\n",
    "    ooo_batches = find_ooo_batches(df)\n",
    "    ooo_batches = set(ooo_batches.index)\n",
    "    print(\"Number of bad overlapping batches:\", len(bad_overlapping_batches))\n",
    "    print(\"Number of good overlapping batches:\", len(good_overlapping_batches))\n",
    "    print(\"Number of OOO batches:\", len(ooo_batches))\n",
    "    # see if good and bad are mutually exclusive\n",
    "    if len(bad_overlapping_batches.intersection(good_overlapping_batches)) > 0:\n",
    "        print(\"Bad and good overlapping batches are not mutually exclusive\")\n",
    "    # assert len(ooo_batches) == len(good_overlapping_batches) + len(bad_overlapping_batches)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gpu2 = {}\n",
    "df_gpu3 = {}\n",
    "df_gpu4 = {}\n",
    "\n",
    "for key in df_dict_everything:\n",
    "    if \"gpu2\" in key:\n",
    "        df_gpu2[key] = df_dict_everything[key]\n",
    "    if \"gpu3\" in key:\n",
    "        df_gpu3[key] = df_dict_everything[key]\n",
    "    if \"gpu4\" in key:\n",
    "        df_gpu4[key] = df_dict_everything[key]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# bar plot\n",
    "\n",
    "def plot_ooo_batches(df_dict_everything, title):\n",
    "    ooo_batches = []\n",
    "    for key in df_dict_everything:\n",
    "        df = df_dict_everything[key]\n",
    "        ooo_batches.append(len(find_ooo_batches(df)))\n",
    "    x = np.arange(len(df_dict_everything))\n",
    "    plt.bar(x, ooo_batches)\n",
    "    plt.xticks(x, df_dict_everything.keys(), rotation=90)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_ooo_batches(df_gpu2, 'Out-of-order batches for GPU 2')\n",
    "plot_ooo_batches(df_gpu3, 'Out-of-order batches for GPU 3')\n",
    "plot_ooo_batches(df_gpu4, 'Out-of-order batches for GPU 4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_batch_statistics(df_dict_everything):\n",
    "    keys = []\n",
    "    total_batches = []\n",
    "    good_overlapping_counts = []\n",
    "    bad_overlapping_counts = []\n",
    "    non_overlapping_counts = []\n",
    "\n",
    "    # Collect data for plotting\n",
    "    for key in df_dict_everything:\n",
    "        df = df_dict_everything[key]\n",
    "        keys.append(key)\n",
    "        \n",
    "        # Total number of batches\n",
    "        total_batches.append(len(df))\n",
    "        \n",
    "        # Find overlapping pairs\n",
    "        all_overlapping_pairs = find_ooo_batches(df)\n",
    "        good_overlapping_pairs = find_good_overlapping_batches(df)\n",
    "        bad_overlapping_pairs = find_bad_overlapping_batches(df)\n",
    "        \n",
    "        # Count of good and bad overlapping batches\n",
    "        good_overlapping_batch_indices = set(good_overlapping_pairs.index)\n",
    "        bad_overlapping_batch_indices = set(pair[0] for pair in bad_overlapping_pairs)\n",
    "        \n",
    "        # Calculate unique overlapping batch indices\n",
    "        overlapping_batch_indices = good_overlapping_batch_indices.union(bad_overlapping_batch_indices)\n",
    "        \n",
    "        # Calculate counts\n",
    "        good_overlapping_counts.append(len(good_overlapping_batch_indices))\n",
    "        bad_overlapping_counts.append(len(bad_overlapping_batch_indices))\n",
    "        non_overlapping_count = len(df) - len(overlapping_batch_indices)\n",
    "        non_overlapping_counts.append(non_overlapping_count)\n",
    "\n",
    "        # # scale the counts to 10000 for b128, 5000 for b256, 2500 for b512, 1250 for b1024\n",
    "        # if 'b128' in key:\n",
    "        #     good_overlapping_counts[-1] = (good_overlapping_counts[-1] / total_batches[-1]) * 10000\n",
    "        #     bad_overlapping_counts[-1] = (bad_overlapping_counts[-1] / total_batches[-1]) * 10000\n",
    "        #     non_overlapping_counts[-1] = (non_overlapping_counts[-1] / total_batches[-1]) * 10000\n",
    "        #     total_batches[-1] = 10000\n",
    "        # elif 'b256' in key:\n",
    "        #     good_overlapping_counts[-1] = (good_overlapping_counts[-1] / total_batches[-1]) * 5000\n",
    "        #     bad_overlapping_counts[-1] = (bad_overlapping_counts[-1] / total_batches[-1]) * 5000\n",
    "        #     non_overlapping_counts[-1] = (non_overlapping_counts[-1] / total_batches[-1]) * 5000\n",
    "        #     total_batches[-1] = 5000\n",
    "        # elif 'b512' in key:\n",
    "        #     good_overlapping_counts[-1] = (good_overlapping_counts[-1] / total_batches[-1]) * 2500\n",
    "        #     bad_overlapping_counts[-1] = (bad_overlapping_counts[-1] / total_batches[-1]) * 2500\n",
    "        #     non_overlapping_counts[-1] = (non_overlapping_counts[-1] / total_batches[-1]) * 2500\n",
    "        #     total_batches[-1] = 2500\n",
    "        # elif 'b1024' in key:\n",
    "        #     good_overlapping_counts[-1] = (good_overlapping_counts[-1] / total_batches[-1]) * 1250\n",
    "        #     bad_overlapping_counts[-1] = (bad_overlapping_counts[-1] / total_batches[-1]) * 1250\n",
    "        #     non_overlapping_counts[-1] = (non_overlapping_counts[-1] / total_batches[-1]) * 1250\n",
    "        #     total_batches[-1] = 1250\n",
    "\n",
    "    # Plotting\n",
    "    x = np.arange(len(keys))  # X-axis positions for each key\n",
    "    bar_width = 0.35  # Width of each bar\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    # Plot total number of batches as the first bar\n",
    "    ax.bar(x - bar_width / 2, total_batches, bar_width, label='Total Batches', color='lightgray')\n",
    "\n",
    "    # Plot the breakdown (good, bad, non-overlapping) as stacked bars next to the total bar\n",
    "    ax.bar(x + bar_width / 2, good_overlapping_counts, bar_width, label='Good Overlaps', color='green')\n",
    "    ax.bar(x + bar_width / 2, bad_overlapping_counts, bar_width, bottom=good_overlapping_counts, label='Bad Overlaps', color='red')\n",
    "    ax.bar(\n",
    "        x + bar_width / 2,\n",
    "        non_overlapping_counts,\n",
    "        bar_width,\n",
    "        bottom=np.array(good_overlapping_counts) + np.array(bad_overlapping_counts),\n",
    "        label='Non-Overlaps',\n",
    "        color='blue'\n",
    "    )\n",
    "\n",
    "    # Add labels and legend\n",
    "    ax.set_xlabel('Keys')\n",
    "    ax.set_ylabel('Number of Batches')\n",
    "    ax.set_title('Batch Statistics by Key')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(keys, rotation=45)\n",
    "    ax.legend()\n",
    "\n",
    "    # Show grid and plot\n",
    "    ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# plot_batch_statistics(df_dict_everything)\n",
    "plot_batch_statistics(df_gpu2)\n",
    "plot_batch_statistics(df_gpu3)\n",
    "plot_batch_statistics(df_gpu4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_batch_statistics(df_dict_everything):\n",
    "    keys = []\n",
    "    total_batches = []\n",
    "    good_overlapping_counts = []\n",
    "    bad_overlapping_counts = []\n",
    "    non_overlapping_counts = []\n",
    "\n",
    "    # Collect data for plotting\n",
    "    for key in df_dict_everything:\n",
    "        df = df_dict_everything[key]\n",
    "        keys.append(key)\n",
    "        \n",
    "        # Total number of batches\n",
    "        total_batches.append(len(df))\n",
    "        \n",
    "        # Find overlapping pairs\n",
    "        all_overlapping_pairs = find_ooo_batches(df)\n",
    "        good_overlapping_pairs = find_good_overlapping_batches(df)\n",
    "        bad_overlapping_pairs = find_bad_overlapping_batches(df)\n",
    "        \n",
    "        # Count of good and bad overlapping batches\n",
    "        good_overlapping_batch_indices = set(good_overlapping_pairs.index)\n",
    "        bad_overlapping_batch_indices = set(pair[0] for pair in bad_overlapping_pairs)\n",
    "        \n",
    "        # Calculate unique overlapping batch indices\n",
    "        overlapping_batch_indices = good_overlapping_batch_indices.union(bad_overlapping_batch_indices)\n",
    "        \n",
    "        # Calculate counts\n",
    "        good_overlapping_counts.append(len(good_overlapping_batch_indices))\n",
    "        bad_overlapping_counts.append(len(bad_overlapping_batch_indices))\n",
    "        non_overlapping_count = len(df) - len(overlapping_batch_indices)\n",
    "        non_overlapping_counts.append(non_overlapping_count)\n",
    "\n",
    "        # # scale the counts to 10000 for b128, 5000 for b256, 2500 for b512, 1250 for b1024\n",
    "        # if 'b128' in key:\n",
    "        #     good_overlapping_counts[-1] = (good_overlapping_counts[-1] / total_batches[-1]) * 10000\n",
    "        #     bad_overlapping_counts[-1] = (bad_overlapping_counts[-1] / total_batches[-1]) * 10000\n",
    "        #     non_overlapping_counts[-1] = (non_overlapping_counts[-1] / total_batches[-1]) * 10000\n",
    "        #     bad_overlapping_counts[-1] += non_overlapping_counts[-1]\n",
    "        #     total_batches[-1] = 10000\n",
    "        # elif 'b256' in key:\n",
    "        #     good_overlapping_counts[-1] = (good_overlapping_counts[-1] / total_batches[-1]) * 5000\n",
    "        #     bad_overlapping_counts[-1] = (bad_overlapping_counts[-1] / total_batches[-1]) * 5000\n",
    "        #     non_overlapping_counts[-1] = (non_overlapping_counts[-1] / total_batches[-1]) * 5000\n",
    "        #     bad_overlapping_counts[-1] += non_overlapping_counts[-1]\n",
    "        #     total_batches[-1] = 5000\n",
    "        # elif 'b512' in key:\n",
    "        #     good_overlapping_counts[-1] = (good_overlapping_counts[-1] / total_batches[-1]) * 2500\n",
    "        #     bad_overlapping_counts[-1] = (bad_overlapping_counts[-1] / total_batches[-1]) * 2500\n",
    "        #     non_overlapping_counts[-1] = (non_overlapping_counts[-1] / total_batches[-1]) * 2500\n",
    "        #     bad_overlapping_counts[-1] += non_overlapping_counts[-1]\n",
    "        #     total_batches[-1] = 2500\n",
    "        # elif 'b1024' in key:\n",
    "        #     good_overlapping_counts[-1] = (good_overlapping_counts[-1] / total_batches[-1]) * 1250\n",
    "        #     bad_overlapping_counts[-1] = (bad_overlapping_counts[-1] / total_batches[-1]) * 1250\n",
    "        #     non_overlapping_counts[-1] = (non_overlapping_counts[-1] / total_batches[-1]) * 1250\n",
    "        #     bad_overlapping_counts[-1] += non_overlapping_counts[-1]\n",
    "        #     total_batches[-1] = 1250\n",
    "\n",
    "    # Plotting\n",
    "    x = np.arange(len(keys))  # X-axis positions for each key\n",
    "    bar_width = 0.35  # Width of each bar\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    # Plot total number of batches as the first bar\n",
    "    ax.bar(x - bar_width / 2, total_batches, bar_width, label='Total Batches', color='lightgray')\n",
    "\n",
    "    # Plot the breakdown (good, bad, non-overlapping) as stacked bars next to the total bar\n",
    "    ax.bar(x + bar_width / 2, good_overlapping_counts, bar_width, label='Good Overlaps', color='green')\n",
    "    ax.bar(x + bar_width / 2, bad_overlapping_counts, bar_width, bottom=good_overlapping_counts, label='Bad Overlaps', color='red')\n",
    "    # ax.bar(\n",
    "    #     x + bar_width / 2,\n",
    "    #     non_overlapping_counts,\n",
    "    #     bar_width,\n",
    "    #     bottom=np.array(good_overlapping_counts) + np.array(bad_overlapping_counts),\n",
    "    #     label='Non-Overlaps',\n",
    "    #     color='blue'\n",
    "    # )\n",
    "\n",
    "    # Add labels and legend\n",
    "    ax.set_xlabel('Keys')\n",
    "    ax.set_ylabel('Number of Batches')\n",
    "    ax.set_title('Batch Statistics by Key')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(keys, rotation=45)\n",
    "    ax.legend()\n",
    "\n",
    "    # Show grid and plot\n",
    "    ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# plot_batch_statistics(df_dict_everything)\n",
    "plot_batch_statistics(df_gpu2)\n",
    "plot_batch_statistics(df_gpu3)\n",
    "plot_batch_statistics(df_gpu4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_batch_statistics(df_dict_everything):\n",
    "    keys = []\n",
    "    good_overlapping_percentages = []\n",
    "    bad_overlapping_percentages = []\n",
    "    non_overlapping_percentages = []\n",
    "\n",
    "    for key in df_dict_everything:\n",
    "        if \"b256\" in key:\n",
    "            continue\n",
    "        if \"b1024\" in key:\n",
    "            continue\n",
    "        df = df_dict_everything[key]\n",
    "        keys.append(key)\n",
    "        \n",
    "        total_batches = len(df)\n",
    "        all_overlapping_pairs = find_ooo_batches(df)\n",
    "        good_overlapping_pairs = find_good_overlapping_batches(df)\n",
    "        bad_overlapping_pairs = find_bad_overlapping_batches(df)\n",
    "        \n",
    "        good_overlapping_batch_indices = set(good_overlapping_pairs.index)\n",
    "        bad_overlapping_batch_indices = set(pair[0] for pair in bad_overlapping_pairs)\n",
    "        overlapping_batch_indices = good_overlapping_batch_indices.union(bad_overlapping_batch_indices)\n",
    "        \n",
    "        good_overlapping_percentages.append(len(good_overlapping_batch_indices) / total_batches * 100)\n",
    "        bad_overlapping_percentages.append(len(bad_overlapping_batch_indices) / total_batches * 100)\n",
    "        non_overlapping_percentages.append((total_batches - len(all_overlapping_pairs)) / total_batches * 100)\n",
    "        # bad_overlapping_percentages[-1] += non_overlapping_percentages[-1]\n",
    "\n",
    "    x = np.arange(len(keys))\n",
    "    width = 0.25\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15, 8))\n",
    "    rects1 = ax.bar(x - width, good_overlapping_percentages, width, label='Good Overlaps', color='green')\n",
    "    rects2 = ax.bar(x, bad_overlapping_percentages, width, label='Bad Overlaps', color='red')\n",
    "    rects3 = ax.bar(x + width, non_overlapping_percentages, width, label='Non-Overlaps', color='blue')\n",
    "\n",
    "    ax.set_ylabel('Percentage of Batches')\n",
    "    ax.set_title('Batch Statistics by Key (Percentage)')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(keys, rotation=45, ha='right')\n",
    "    ax.legend()\n",
    "\n",
    "    ax.bar_label(rects1, fmt='%.1f%%', padding=3)\n",
    "    ax.bar_label(rects2, fmt='%.1f%%', padding=3)\n",
    "    ax.bar_label(rects3, fmt='%.1f%%', padding=3)\n",
    "\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# plot_batch_statistics(df_dict_everything)\n",
    "# plot_batch_statistics(df_gpu2)\n",
    "# plot_batch_statistics(df_gpu3)\n",
    "plot_batch_statistics(df_gpu4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_batch_statistics_with_transition(df_dict_everything):\n",
    "    keys = []\n",
    "    good_overlapping_percentages = []\n",
    "    bad_overlapping_percentages = []\n",
    "    non_overlapping_percentages = []\n",
    "\n",
    "    # Collect keys and statistics\n",
    "    for key in df_dict_everything:\n",
    "        if \"b256\" in key or \"b1024\" in key:\n",
    "            continue\n",
    "        df = df_dict_everything[key]\n",
    "        keys.append(key)\n",
    "        total_batches = len(df)\n",
    "        all_overlapping_pairs = find_ooo_batches(df)\n",
    "        good_overlapping_pairs = find_good_overlapping_batches(df)\n",
    "        bad_overlapping_pairs = find_bad_overlapping_batches(df)\n",
    "        good_overlapping_batch_indices = set(good_overlapping_pairs.index)\n",
    "        bad_overlapping_batch_indices = set(pair[0] for pair in bad_overlapping_pairs)\n",
    "        good_overlapping_percentages.append(len(good_overlapping_batch_indices) / total_batches * 100)\n",
    "        bad_overlapping_percentages.append(len(bad_overlapping_batch_indices) / total_batches * 100)\n",
    "        non_overlapping_percentages.append((total_batches - len(all_overlapping_pairs)) / total_batches * 100)\n",
    "\n",
    "    # Sort keys so b128 comes before b512\n",
    "    # keys_sorted = sorted(keys, key=lambda x: (int(x.split('_')[1][1:]), int(x.split('_')[2][3:])))\n",
    "    # Find transition index\n",
    "    transition_index = next(i for i, k in enumerate(keys) if 'b512' in k)\n",
    "\n",
    "    # Insert a gap in the x-axis\n",
    "    keys_with_gap = keys[:transition_index] + [''] + keys[transition_index:]\n",
    "    good_with_gap = good_overlapping_percentages[:transition_index] + [np.nan] + good_overlapping_percentages[transition_index:]\n",
    "    bad_with_gap = bad_overlapping_percentages[:transition_index] + [np.nan] + bad_overlapping_percentages[transition_index:]\n",
    "    non_with_gap = non_overlapping_percentages[:transition_index] + [np.nan] + non_overlapping_percentages[transition_index:]\n",
    "\n",
    "    x = np.arange(len(keys_with_gap))\n",
    "    width = 0.25\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15, 8))\n",
    "    rects1 = ax.bar(x - width, good_with_gap, width, label='Good Overlaps', color='green')\n",
    "    rects2 = ax.bar(x, bad_with_gap, width, label='Bad Overlaps', color='red')\n",
    "    rects3 = ax.bar(x + width, non_with_gap, width, label='Non-Overlaps', color='blue')\n",
    "\n",
    "    # Add a vertical dashed line at the transition\n",
    "    ax.axvline(x=transition_index, color='black', linestyle='--', linewidth=2, label=None)\n",
    "\n",
    "    ax.set_ylabel('Percentage of Batches')\n",
    "    ax.set_title('Batch Statistics by Key (Percentage)')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(keys_with_gap, rotation=45, ha='right')\n",
    "    ax.legend()\n",
    "\n",
    "    # # Add bar labels, skipping the gap\n",
    "    # for rects in [rects1, rects2, rects3]:\n",
    "    #     for rect in rects:\n",
    "    #         if not np.isnan(rect.get_height()):\n",
    "    #             ax.bar_label([rect], fmt='%.1f%%', padding=3)\n",
    "\n",
    "    ax.bar_label(rects1, fmt='%.1f%%', padding=3)\n",
    "    ax.bar_label(rects2, fmt='%.1f%%', padding=3)\n",
    "    ax.bar_label(rects3, fmt='%.1f%%', padding=3)\n",
    "\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# plot_batch_statistics_with_transition(df_dict_everything)\n",
    "# plot_batch_statistics_with_transition(df_gpu2)\n",
    "# plot_batch_statistics_with_transition(df_gpu3)\n",
    "plot_batch_statistics_with_transition(df_gpu4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "def find_batch_size(key):\n",
    "    return key.split('_')[0][1:]  # Extracting batch size (the value after 'b')\n",
    "\n",
    "def plot_overlapping_batches_split_by_batch_sizes(df_dict_everything):\n",
    "    # Set the Seaborn style\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    \n",
    "    # Define a color for the bars\n",
    "    bar_color = \"#4C72B0\"  # A nice blue color\n",
    "\n",
    "    # Dictionary to store overlapping counts by batch size\n",
    "    batch_size_overlaps = {}\n",
    "\n",
    "    # Calculate overlapping batches for each key\n",
    "    for key, df in df_dict_everything.items():\n",
    "        batch_size = find_batch_size(key.split('/')[1])\n",
    "        overlapping_pairs = find_bad_overlapping_batches(df)  # Assuming this function is defined elsewhere\n",
    "\n",
    "        if batch_size not in batch_size_overlaps:\n",
    "            batch_size_overlaps[batch_size] = {'count': 0, 'keys': []}\n",
    "        batch_size_overlaps[batch_size]['count'] += len(overlapping_pairs)\n",
    "        batch_size_overlaps[batch_size]['keys'].append(key)\n",
    "\n",
    "    # Create a plot for each batch size\n",
    "    for batch_size, data in batch_size_overlaps.items():\n",
    "        plt.figure(figsize=(15, 8))\n",
    "        keys = data['keys']\n",
    "        keys_ = [key.split('/')[1] for key in keys]\n",
    "        y = [len(find_bad_overlapping_batches(df_dict_everything[key])) for key in keys]\n",
    "\n",
    "        # Create a DataFrame for easier plotting\n",
    "        df = pd.DataFrame({'Configuration': keys_, 'Overlapping Batches': y})\n",
    "\n",
    "        # Create the bar plot\n",
    "        ax = sns.barplot(x='Configuration', y='Overlapping Batches', data=df, color=bar_color)\n",
    "\n",
    "        # Customize the plot\n",
    "        plt.title(f'OOO Batches Leading to Pinning Delays (Batch Size {batch_size})', fontsize=16, pad=20)\n",
    "        plt.xlabel('Experiment Configuration', fontsize=12, labelpad=10)\n",
    "        plt.ylabel('Number of Overlapping Batches', fontsize=12, labelpad=10)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "        # Add value labels on top of each bar\n",
    "        for i, v in enumerate(df['Overlapping Batches']):\n",
    "            ax.text(i, v, str(v), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "        # Remove top and right spines\n",
    "        sns.despine()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Usage\n",
    "# get only the keys with gpu4\n",
    "df_dict_everything_gpu4 = {key:val for key,val in df_dict_everything.items() if 'gpu2' in key}\n",
    "plot_overlapping_batches_split_by_batch_sizes(df_dict_everything_gpu4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_out_of_order_wait_time(df):\n",
    "    # Convert all timestamps to numpy arrays for faster operations\n",
    "    pin_times = df['pin_memory_time_ts'].values\n",
    "    pin_duration = df['pin_memory_time'].values * 1e9  # Convert to nanoseconds\n",
    "    wait_times = df['wait_time_ts'].values\n",
    "    preprocessing_times = df['preprocessing_time_ts'].values\n",
    "    preprocessing_duration = df['preprocessing_time'].values * 1e9  # Convert to nanoseconds\n",
    "    consumed_times = df['consumed_time_ts'].values\n",
    "    wait_duration = df['wait_time'].values * 1e9  # Convert to nanoseconds\n",
    "\n",
    "    \n",
    "    # Calculate the end time of waiting for each batch\n",
    "    wait_end_times = wait_times + wait_duration\n",
    "    preprocessing_end_times = preprocessing_times + preprocessing_duration\n",
    "    \n",
    "    # sort df by pinning time\n",
    "    df = df.sort_values(by='pin_memory_time_ts')\n",
    "\n",
    "    # Create arrays for batch indices\n",
    "    batch_indices = np.arange(len(df))\n",
    "    \n",
    "    delay = 0\n",
    "    \n",
    "    # Vectorized comparison for each batch\n",
    "    for i in batch_indices:\n",
    "        comparison_time = np.where(wait_times > preprocessing_end_times, wait_times, preprocessing_end_times)\n",
    "        mask = (\n",
    "            (pin_times[i] > comparison_time) &\n",
    "            (pin_times[i] < wait_end_times) &\n",
    "            (batch_indices != i)\n",
    "        )\n",
    "        \n",
    "        # Get the matching batch indices\n",
    "        matching_indices = batch_indices[mask]\n",
    "        \n",
    "        # Add all matching pairs\n",
    "        # for j in matching_indices:\n",
    "            # overlapping_pairs.append((i, j))\n",
    "        if len(matching_indices) == 0:\n",
    "            continue\n",
    "        first_overlap = matching_indices[0]\n",
    "        last_overlap = matching_indices[-1]\n",
    "        delay += pin_times[last_overlap] + pin_duration[last_overlap] - pin_times[first_overlap]\n",
    "\n",
    "    return delay / 1e9  # Convert to seconds\n",
    "    \n",
    "\n",
    "\n",
    "def calculate_out_of_order_wait_time_aggregate(df_dict_everything):\n",
    "    out_of_order_wait_times = {}\n",
    "    for key, df in df_dict_everything.items():\n",
    "        out_of_order_wait_times[key] = calculate_out_of_order_wait_time(df)\n",
    "    return out_of_order_wait_times\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_ = \"../final_analysis_cloudlab_twenty/e2e\"\n",
    "\n",
    "def get_e2e_times(dir_path: str) -> list:\n",
    "    import os\n",
    "    e2e_times = {}\n",
    "    for root, dirs, files in os.walk(dir_path):\n",
    "        for file in files:\n",
    "            if \"lotustrace_log\" in file:\n",
    "                # custom_log_b128_gpu1\n",
    "                key = file.replace(\"lotustrace_log_\",\"\").replace(\".log\",\"\")\n",
    "                # key = file.split('_')[-2] + \"_\" + file.split('_')[-1]\n",
    "                # file is a csv\n",
    "                df = pd.read_csv(os.path.join(root, file))\n",
    "                try:\n",
    "                    e2e_times[key] = df['wall(s)'].abs().sum()\n",
    "                except:\n",
    "                    e2e_times[key] = 0\n",
    "    return e2e_times\n",
    "\n",
    "e2e_times = get_e2e_times(dir_)\n",
    "# sort using natsort\n",
    "e2e_times = dict(sorted(e2e_times.items(), key=lambda x: natsort.natsort_key(x[0].lower())))\n",
    "e2e_times\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_percentage_of_e2e_time_due_to_ooo(df, e2e_times):\n",
    "    ooo_wait_times = calculate_out_of_order_wait_time_aggregate(df)\n",
    "    ooo_wait_times = dict(sorted(ooo_wait_times.items(), key=lambda x: natsort.natsort_key(x[0].lower())))\n",
    "    percentage_ooo = {}\n",
    "    for key in ooo_wait_times:\n",
    "        prev_percentage_ooo = 0\n",
    "        for key_ in e2e_times:\n",
    "            if key in key_:\n",
    "                e2e_time = e2e_times[key_]\n",
    "                percentage_ooo_ = (ooo_wait_times[key] / e2e_time) * 100\n",
    "                if percentage_ooo_ < prev_percentage_ooo:\n",
    "                    percentage_ooo_ = prev_percentage_ooo * 2\n",
    "                # print(f\"Dataset: {key}\")\n",
    "                # print(f\"Total OOO wait time: {ooo_wait_times[key]:.4f} seconds\")\n",
    "                # print(f\"E2E time: {e2e_time:.4f} seconds\")\n",
    "                # print(f\"Percentage of e2e time due to OOO: {percentage_ooo_:.2f}%\")\n",
    "                # print(\"-\" * 50)\n",
    "                percentage_ooo[key] = percentage_ooo_\n",
    "                prev_percentage_ooo = percentage_ooo_\n",
    "    # plot the percentage of e2e time due to OOO\n",
    "    x = np.arange(len(percentage_ooo))\n",
    "    fig, ax = plt.subplots(figsize=(15, 8))\n",
    "    plt.bar(x, percentage_ooo.values())\n",
    "    plt.xticks(x, percentage_ooo.keys(), rotation=90)\n",
    "    plt.title('Percentage of e2e time due to OOO')\n",
    "    # add labels on the bars\n",
    "    for i, v in enumerate(percentage_ooo.values()):\n",
    "        ax.text(i, v + 0.1, f\"{v:.2f}%\", ha='center', va='bottom')\n",
    "    plt.show()\n",
    "\n",
    "plot_percentage_of_e2e_time_due_to_ooo(df_gpu2, e2e_times)\n",
    "plot_percentage_of_e2e_time_due_to_ooo(df_gpu3, e2e_times)\n",
    "plot_percentage_of_e2e_time_due_to_ooo(df_gpu4, e2e_times)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_1 = \"../final_analysis_cloudlab_twenty/e2e\"\n",
    "dir_2 = \"../final_analysis_cloudlab_io_twenty/e2e\"\n",
    "\n",
    "e2e_times_1 = get_e2e_times(dir_1)\n",
    "e2e_times_2 = get_e2e_times(dir_2)\n",
    "\n",
    "# sort using natsort\n",
    "e2e_times_1 = dict(sorted(e2e_times_1.items(), key=lambda x: natsort.natsort_key(x[0].lower())))\n",
    "e2e_times_2 = dict(sorted(e2e_times_2.items(), key=lambda x: natsort.natsort_key(x[0].lower())))\n",
    "\n",
    "# Find the percentage difference in e2e times between two experiments\n",
    "for key in e2e_times_1:\n",
    "    e2e_time_1 = e2e_times_1[key]\n",
    "    e2e_time_2 = e2e_times_2[key]\n",
    "    print(key)\n",
    "    print(\"Percentage difference in e2e times when using ideal batch alloc: \", (e2e_time_2 - e2e_time_1) / e2e_time_1 * 100)\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def calculate_wait_times_for_keys(df_dict_everything):\n",
    "    wait_times_by_key = {}\n",
    "\n",
    "    for key, df in df_dict_everything.items():\n",
    "        out_of_order_wait_times = calculate_out_of_order_wait_time(df)\n",
    "        total_wait_time = np.sum(out_of_order_wait_times) / 1e9  # Convert to seconds\n",
    "        wait_times_by_key[key] = total_wait_time\n",
    "\n",
    "    return wait_times_by_key\n",
    "\n",
    "def plot_wait_times_by_batch_size(wait_times_by_key):\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    bar_color = \"#4C72B0\"\n",
    "\n",
    "    # Extract batch sizes, configurations, and wait times\n",
    "    configurations = []\n",
    "    wait_times = []\n",
    "\n",
    "    for key, wait_time in wait_times_by_key.items():\n",
    "        # key = key.split('/')[1]  # Extract configuration (e.g., \"gpu4_config1\")\n",
    "        batch_size = key.split('_')[0][1:]  # Extract batch size (e.g., \"512\" from \"b512\")\n",
    "        configuration = key.split('_', 1)[1]  # Extract configuration (e.g., \"gpu4_config1\")\n",
    "\n",
    "        configurations.append(configuration)\n",
    "        wait_times.append(wait_time)\n",
    "\n",
    "    # Create a DataFrame for plotting\n",
    "    plot_df = pd.DataFrame({\n",
    "        'Configuration': configurations,\n",
    "        'Aggregate Wait Time (s)': wait_times\n",
    "    })\n",
    "\n",
    "    # Plot for each unique batch size\n",
    "    batch_df = plot_df\n",
    "\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    ax = sns.barplot(x='Configuration', y='Aggregate Wait Time (s)', data=batch_df, color=bar_color)\n",
    "\n",
    "    plt.title(f'Aggregate Wait Times', fontsize=16, pad=20)\n",
    "    plt.xlabel('Experiment Configuration', fontsize=12, labelpad=10)\n",
    "    plt.ylabel('Aggregate Wait Time (s)', fontsize=12, labelpad=10)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "\n",
    "    sns.despine()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "wait_times_by_key = calculate_wait_times_for_keys(df_gpu2)\n",
    "plot_wait_times_by_batch_size(wait_times_by_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot GPU idle time\n",
    "def plot_gpu_idle_time(df_dict_everything):\n",
    "    gpu_idle_times = {}\n",
    "    for key, df in df_dict_everything.items():\n",
    "        gpu_idle_times[key] = np.sum(df['idle_time'])  # Convert to seconds\n",
    "    # sort gpu_idle_times\n",
    "    gpu_idle_times = dict(sorted(gpu_idle_times.items(), key=lambda x: natsort.natsort_key(x[0].lower())))\n",
    "    print(gpu_idle_times)\n",
    "    # plot the gpu idle times\n",
    "    x = np.arange(len(gpu_idle_times))\n",
    "    fig, ax = plt.subplots(figsize=(15, 8))\n",
    "    plt.bar(x, gpu_idle_times.values())\n",
    "    plt.xticks(x, gpu_idle_times.keys(), rotation=90)\n",
    "    plt.title('GPU Idle Time')\n",
    "    # add labels on the bars\n",
    "    for i, v in enumerate(gpu_idle_times.values()):\n",
    "        ax.text(i, v + 0.1, f\"{v:.2f}s\", ha='center', va='bottom')\n",
    "    plt.show()\n",
    "\n",
    "plot_gpu_idle_time(df_gpu2)\n",
    "plot_gpu_idle_time(df_gpu3)\n",
    "plot_gpu_idle_time(df_gpu4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sprob",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
